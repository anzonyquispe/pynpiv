{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npiv_default(\n",
    "    Y: Iterable[float],\n",
    "    X: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    X_eval: Optional[np.ndarray] = None,\n",
    "    X_grid: Optional[np.ndarray] = None,\n",
    "    alpha: float = 0.05,\n",
    "    basis: str = \"tensor\",                 # options: \"tensor\", \"additive\", \"glp\"\n",
    "    boot_num: int = 99,\n",
    "    check_is_fullrank: bool = False,\n",
    "    deriv_index: int = 1,\n",
    "    deriv_order: int = 1,\n",
    "    grid_num: int = 50,\n",
    "    J_x_degree: int = 3,\n",
    "    J_x_segments: Optional[int] = None,\n",
    "    K_w_degree: int = 4,\n",
    "    K_w_segments: Optional[int] = None,\n",
    "    K_w_smooth: int = 2,\n",
    "    knots: str = \"uniform\",                # or \"quantiles\"\n",
    "    progress: bool = True,\n",
    "    ucb_h: bool = True,\n",
    "    ucb_deriv: bool = True,\n",
    "    W_max: Optional[float] = None,\n",
    "    W_min: Optional[float] = None,\n",
    "    X_min: Optional[float] = None,\n",
    "    X_max: Optional[float] = None,\n",
    "    *,\n",
    "    npiv_est: Optional[Callable[..., Dict[str, Any]]] = None,\n",
    "    **kwargs: Any,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python port of `npiv.default`. This is a thin wrapper around a user-supplied\n",
    "    estimation routine `npiv_est(...)` that should implement the actual NPIV fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y, X, W : arrays\n",
    "        Data for outcome Y, regressors X, instruments W.\n",
    "    X_eval, X_grid : arrays, optional\n",
    "        Evaluation points and grid (if your estimator supports them).\n",
    "    alpha, basis, boot_num, ... : hyperparameters\n",
    "        Passed through to `npiv_est`.\n",
    "    npiv_est : callable\n",
    "        Function implementing the estimator. Must accept named arguments used below\n",
    "        and return a dict-like object with results. Example signature:\n",
    "\n",
    "            def npiv_est(Y, X, W, X_eval=None, X_grid=None, alpha=0.05, basis=\"tensor\",\n",
    "                         boot_num=99, check_is_fullrank=False, deriv_index=1, deriv_order=1,\n",
    "                         grid_num=50, J_x_degree=3, J_x_segments=None, K_w_degree=4,\n",
    "                         K_w_segments=None, K_w_smooth=2, knots=\"uniform\", progress=True,\n",
    "                         ucb_h=True, ucb_deriv=True, W_max=None, W_min=None, X_min=None, X_max=None, **kwargs) -> dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    est : dict\n",
    "        The estimator's output, augmented with metadata:\n",
    "        - 'call'  : string describing the call\n",
    "        - 'ptm'   : elapsed time in seconds (float)\n",
    "        - repeats of key hyperparameters for convenience (alpha, basis, ...)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function does not implement NPIV itself; provide `npiv_est`.\n",
    "    - Naming is snake_case in Python; it corresponds to the R names one-to-one.\n",
    "    \"\"\"\n",
    "    if npiv_est is None:\n",
    "        raise NotImplementedError(\n",
    "            \"Please provide an `npiv_est` callable (the actual estimator). \"\n",
    "            \"This wrapper mirrors R's npiv.default and attaches metadata.\"\n",
    "        )\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Call the user-provided estimator\n",
    "    est = npiv_est(\n",
    "        Y=Y,\n",
    "        X=X,\n",
    "        W=W,\n",
    "        X_eval=X_eval,\n",
    "        X_grid=X_grid,\n",
    "        alpha=alpha,\n",
    "        basis=basis,\n",
    "        boot_num=boot_num,\n",
    "        check_is_fullrank=check_is_fullrank,\n",
    "        deriv_index=deriv_index,\n",
    "        deriv_order=deriv_order,\n",
    "        grid_num=grid_num,\n",
    "        J_x_degree=J_x_degree,\n",
    "        J_x_segments=J_x_segments,\n",
    "        K_w_degree=K_w_degree,\n",
    "        K_w_segments=K_w_segments,\n",
    "        K_w_smooth=K_w_smooth,\n",
    "        knots=knots,\n",
    "        progress=progress,\n",
    "        ucb_h=ucb_h,\n",
    "        ucb_deriv=ucb_deriv,\n",
    "        W_max=W_max,\n",
    "        W_min=W_min,\n",
    "        X_min=X_min,\n",
    "        X_max=X_max,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Augment with metadata (parity with R code)\n",
    "    call_str = (\n",
    "        \"npiv_default(Y, X, W, \"\n",
    "        f\"X_eval={X_eval is not None}, X_grid={X_grid is not None}, \"\n",
    "        f\"alpha={alpha}, basis='{basis}', boot_num={boot_num}, \"\n",
    "        f\"check_is_fullrank={check_is_fullrank}, deriv_index={deriv_index}, deriv_order={deriv_order}, \"\n",
    "        f\"grid_num={grid_num}, J_x_degree={J_x_degree}, J_x_segments={J_x_segments}, \"\n",
    "        f\"K_w_degree={K_w_degree}, K_w_segments={K_w_segments}, K_w_smooth={K_w_smooth}, \"\n",
    "        f\"knots='{knots}', progress={progress}, ucb_h={ucb_h}, ucb_deriv={ucb_deriv}, \"\n",
    "        f\"W_max={W_max}, W_min={W_min}, X_min={X_min}, X_max={X_max})\"\n",
    "    )\n",
    "\n",
    "    # Ensure dict-like result\n",
    "    if not isinstance(est, dict):\n",
    "        # Allow objects with __dict__\n",
    "        try:\n",
    "            est = dict(est)\n",
    "        except Exception:\n",
    "            est = {\"result\": est}\n",
    "\n",
    "    est.update(\n",
    "        {\n",
    "            \"call\": call_str,\n",
    "            \"ptm\": (t1 - t0),  # seconds\n",
    "            \"alpha\": alpha,\n",
    "            \"basis\": basis,\n",
    "            \"boot_num\": boot_num,\n",
    "            \"check_is_fullrank\": check_is_fullrank,\n",
    "            \"grid_num\": grid_num,\n",
    "            \"knots\": knots,\n",
    "            \"progress\": progress,\n",
    "            \"W_max\": W_max,\n",
    "            \"W_min\": W_min,\n",
    "            \"X_min\": X_min,\n",
    "            \"X_max\": X_max,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # In R they set class(est) <- \"npiv\"; in Python just annotate:\n",
    "    est.setdefault(\"_class\", \"npiv\")\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7442bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- main function ---------\n",
    "\n",
    "def npiv_est(\n",
    "    Y: Iterable[float],\n",
    "    X: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    X_eval: Optional[np.ndarray] = None,\n",
    "    X_grid: Optional[np.ndarray] = None,     # passed to chooser if needed\n",
    "    alpha: float = 0.05,\n",
    "    basis: str = \"tensor\",                   # {\"tensor\",\"additive\",\"glp\"}\n",
    "    boot_num: int = 99,\n",
    "    check_is_fullrank: bool = False,\n",
    "    deriv_index: int = 1,\n",
    "    deriv_order: int = 1,\n",
    "    grid_num: int = 50,\n",
    "    J_x_degree: int = 3,\n",
    "    J_x_segments: Optional[int] = None,\n",
    "    K_w_degree: int = 4,\n",
    "    K_w_segments: Optional[int] = None,\n",
    "    K_w_smooth: int = 2,\n",
    "    knots: str = \"uniform\",                  # {\"uniform\",\"quantiles\"}\n",
    "    progress: bool = True,\n",
    "    ucb_h: bool = True,\n",
    "    ucb_deriv: bool = True,\n",
    "    W_max: Optional[float] = None,\n",
    "    W_min: Optional[float] = None,\n",
    "    X_max: Optional[float] = None,\n",
    "    X_min: Optional[float] = None,\n",
    "    *,\n",
    "    prodspline: Callable[..., np.ndarray],\n",
    "    npiv_choose_J: Optional[Callable[..., Dict[str, Any]]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python port of npivEst. Requires:\n",
    "      - prodspline(...) -> ndarray   (basis builder)\n",
    "      - npiv_choose_J(...) -> dict   (only if data-driven J/K selection is needed)\n",
    "    \"\"\"\n",
    "    basis = _match_arg(basis, [\"tensor\", \"additive\", \"glp\"], \"basis\")\n",
    "    knots = _match_arg(knots, [\"uniform\", \"quantiles\"], \"knots\")\n",
    "\n",
    "    if Y is None: raise ValueError(\"must provide Y\")\n",
    "    if X is None: raise ValueError(\"must provide X\")\n",
    "    if W is None: raise ValueError(\"must provide W\")\n",
    "    if K_w_degree < 0: raise ValueError(\"K.w.degree must be a non-negative integer\")\n",
    "    if J_x_degree < 0: raise ValueError(\"J.x.degree must be a non-negative integer\")\n",
    "    if K_w_segments is not None and K_w_segments <= 0: raise ValueError(\"K.w.segments must be positive\")\n",
    "    if J_x_segments is not None and J_x_segments <= 0: raise ValueError(\"J.x.segments must be positive\")\n",
    "\n",
    "    Y = _as_colvec(Y)\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    W = np.asarray(W, dtype=float)\n",
    "\n",
    "    if check_is_fullrank:\n",
    "        if not _is_fullrank(Y): raise ValueError(\"Y is not of full column rank\")\n",
    "        if not _is_fullrank(X): raise ValueError(\"X is not of full column rank\")\n",
    "        if not _is_fullrank(W): raise ValueError(\"W is not of full column rank\")\n",
    "\n",
    "    # -------- regression vs IV; decide data-driven selection ----------\n",
    "    data_driven = False\n",
    "    if _identical(X, W):\n",
    "        # regression: if J unspecified -> data-driven\n",
    "        if J_x_segments is None:\n",
    "            data_driven = True\n",
    "        K_w_degree = J_x_degree\n",
    "        # if J is specified, match degrees/segments; otherwise chooser will set\n",
    "        if J_x_segments is not None:\n",
    "            K_w_segments = J_x_segments\n",
    "        K_w_smooth = 0\n",
    "    else:\n",
    "        # IV: if either J or K missing -> data-driven\n",
    "        if K_w_segments is None or J_x_segments is None:\n",
    "            data_driven = True\n",
    "\n",
    "    if data_driven:\n",
    "        if npiv_choose_J is None:\n",
    "            raise ValueError(\"Data-driven selection requested but `npiv_choose_J` was not provided.\")\n",
    "        test1 = npiv_choose_J(\n",
    "            Y=Y, X=X, W=W, X_grid=X_grid,\n",
    "            J_x_degree=J_x_degree, K_w_degree=K_w_degree, K_w_smooth=K_w_smooth,\n",
    "            knots=knots, basis=basis,\n",
    "            X_min=X_min, X_max=X_max, W_min=W_min, W_max=W_max,\n",
    "            grid_num=grid_num, boot_num=boot_num,\n",
    "            check_is_fullrank=check_is_fullrank, progress=progress,\n",
    "            prodspline=prodspline,  # forwarded to internals\n",
    "            dimbs=None,             # the chooser itself will pass its own if needed\n",
    "        )\n",
    "        K_w_segments = int(test1[\"K.w.seg\"])\n",
    "        J_x_segments = int(test1[\"J.x.seg\"])\n",
    "        J_x_segments_set = test1[\"J.x.segments.set\"]\n",
    "        K_w_segments_set = test1[\"K.w.segments.set\"]\n",
    "    else:\n",
    "        J_x_segments_set = K_w_segments_set = None\n",
    "\n",
    "    if K_w_degree + (K_w_segments or 0) < J_x_degree + (J_x_segments or 0):\n",
    "        raise ValueError(\"K.w.degree+K.w.segments must be >= J.x.degree+J.x.segments\")\n",
    "\n",
    "    # -------- Build W basis B.w --------\n",
    "    if K_w_degree == 0:\n",
    "        B_w = np.ones((W.shape[0], 1))\n",
    "    else:\n",
    "        K_W = np.column_stack([\n",
    "            np.repeat(K_w_degree, W.shape[1]),\n",
    "            np.repeat(K_w_segments, W.shape[1]),\n",
    "        ])\n",
    "        B_w = prodspline(x=W, K=K_W, knots=knots, basis=basis, x_min=W_min, x_max=W_max)\n",
    "        if basis != \"tensor\":\n",
    "            B_w = np.column_stack([np.ones((W.shape[0], 1)), B_w])\n",
    "\n",
    "    # -------- Build X basis Psi.x (+ derivative, and eval versions) --------\n",
    "    if J_x_degree == 0:\n",
    "        Psi_x = np.ones((X.shape[0], 1))\n",
    "        Psi_x_eval = Psi_x if X_eval is None else np.ones((X_eval.shape[0], 1))\n",
    "        Psi_x_deriv = np.zeros_like(Psi_x)\n",
    "        Psi_x_deriv_eval = Psi_x_deriv if X_eval is None else np.zeros_like(Psi_x_eval)\n",
    "    else:\n",
    "        K_X = np.column_stack([\n",
    "            np.repeat(J_x_degree, X.shape[1]),\n",
    "            np.repeat(J_x_segments, X.shape[1]),\n",
    "        ])\n",
    "        Psi_x = prodspline(x=X, K=K_X, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "        Psi_x_deriv = prodspline(\n",
    "            x=X, K=K_X, knots=knots, basis=basis,\n",
    "            deriv_index=deriv_index, deriv=deriv_order,\n",
    "            x_min=X_min, x_max=X_max\n",
    "        )\n",
    "        if X_eval is None:\n",
    "            Psi_x_eval = Psi_x\n",
    "            Psi_x_deriv_eval = Psi_x_deriv\n",
    "        else:\n",
    "            Psi_x_eval = prodspline(x=X, xeval=X_eval, K=K_X, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "            Psi_x_deriv_eval = prodspline(\n",
    "                x=X, xeval=X_eval, K=K_X, knots=knots, basis=basis,\n",
    "                deriv_index=deriv_index, deriv=deriv_order,\n",
    "                x_min=X_min, x_max=X_max\n",
    "            )\n",
    "        if basis != \"tensor\":\n",
    "            Psi_x = np.column_stack([np.ones((Psi_x.shape[0], 1)), Psi_x])\n",
    "            Psi_x_eval = np.column_stack([np.ones((Psi_x_eval.shape[0], 1)), Psi_x_eval])\n",
    "            Psi_x_deriv = np.column_stack([np.zeros((Psi_x_deriv.shape[0], 1)), Psi_x_deriv])\n",
    "            Psi_x_deriv_eval = np.column_stack([np.zeros((Psi_x_deriv_eval.shape[0], 1)), Psi_x_deriv_eval])\n",
    "\n",
    "    # -------- 2SLS algebra (robust to large bases via pinv) --------\n",
    "    # tmp = (Psi' B (B'B)^-1 B')^+ Psi' B (B'B)^-1 B'\n",
    "    BtB_inv = _ginv(B_w.T @ B_w)\n",
    "    PsiTPB = Psi_x.T @ B_w @ BtB_inv @ B_w.T\n",
    "    tmp = _ginv(PsiTPB @ Psi_x) @ PsiTPB\n",
    "    beta = tmp @ Y  # (J x 1)\n",
    "\n",
    "    # -------- fitted function and derivative at eval data --------\n",
    "    h = Psi_x_eval @ beta\n",
    "    h_deriv = Psi_x_deriv_eval @ beta\n",
    "\n",
    "    # -------- asymptotic SEs --------\n",
    "    U_hat = Y - (Psi_x @ beta)                # n x 1\n",
    "    Wgt = (tmp.T * U_hat.ravel()[:, None])    # n x J\n",
    "    D = Wgt.T @ Wgt                           # J x J\n",
    "\n",
    "    asy_se = np.sqrt(np.abs(np.sum((Psi_x_eval @ D) * Psi_x_eval, axis=1)))          # (neval,)\n",
    "    asy_se_deriv = np.sqrt(np.abs(np.sum((Psi_x_deriv_eval @ D) * Psi_x_deriv_eval, axis=1)))\n",
    "\n",
    "    # -------- UCBs (two regimes) --------\n",
    "    h_lower = h_upper = cv = None\n",
    "    h_lower_deriv = h_upper_deriv = cv_deriv = None\n",
    "\n",
    "    if ucb_h or ucb_deriv:\n",
    "        rng = np.random.default_rng(12345)  # fixed to mimic with_preserve_seed behavior\n",
    "        n = X.shape[0]\n",
    "        boot_Z_h = None\n",
    "        boot_Z_d = None\n",
    "\n",
    "        if data_driven:\n",
    "            # Build per-J bootstrap using the chooser’s grid\n",
    "            J_set = J_x_segments_set\n",
    "            K_set = K_w_segments_set\n",
    "            if J_set is None or K_set is None:\n",
    "                raise ValueError(\"Data-driven UCB requires J.x.segments.set and K.w.segments.set.\")\n",
    "\n",
    "            # Optional truncation like in R\n",
    "            if len(J_set) > 2:\n",
    "                cutoff = max(J_x_segments, max(J_set[:-2]))\n",
    "                J_boot = J_set[J_set <= cutoff]\n",
    "                K_boot = K_set[: len(J_boot)]\n",
    "            else:\n",
    "                J_boot, K_boot = J_set, K_set\n",
    "\n",
    "            if ucb_h: boot_Z_h = np.full((boot_num, len(J_boot)), np.nan)\n",
    "            if ucb_deriv: boot_Z_d = np.full((boot_num, len(J_boot)), np.nan)\n",
    "\n",
    "            # Pre-generate the same bootstrap draws used for every J\n",
    "            Zmat = rng.standard_normal(size=(boot_num, n))\n",
    "\n",
    "            for ii in range(len(J_boot)):\n",
    "                J_seg = int(J_boot[ii])\n",
    "                K_seg = int(K_boot[ii])\n",
    "\n",
    "                # W basis at this J\n",
    "                if K_w_degree == 0:\n",
    "                    B_w_J = np.ones((W.shape[0], 1))\n",
    "                else:\n",
    "                    K_WJ = np.column_stack([\n",
    "                        np.repeat(K_w_degree, W.shape[1]),\n",
    "                        np.repeat(K_seg, W.shape[1]),\n",
    "                    ])\n",
    "                    B_w_J = prodspline(x=W, K=K_WJ, knots=knots, basis=basis, x_min=W_min, x_max=W_max)\n",
    "                    if basis != \"tensor\":\n",
    "                        B_w_J = np.column_stack([np.ones((W.shape[0], 1)), B_w_J])\n",
    "\n",
    "                # X bases at this J\n",
    "                if J_x_degree == 0:\n",
    "                    Psi_x_J = np.ones((X.shape[0], 1))\n",
    "                    Psi_x_J_eval = Psi_x_J if X_eval is None else np.ones((X_eval.shape[0], 1))\n",
    "                    Psi_x_J_deriv_eval = np.zeros_like(Psi_x_J_eval)\n",
    "                else:\n",
    "                    K_XJ = np.column_stack([\n",
    "                        np.repeat(J_x_degree, X.shape[1]),\n",
    "                        np.repeat(J_seg, X.shape[1]),\n",
    "                    ])\n",
    "                    Psi_x_J = prodspline(x=X, K=K_XJ, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "                    Psi_x_J_eval = Psi_x_J if X_eval is None else prodspline(\n",
    "                        x=X, xeval=X_eval, K=K_XJ, knots=knots, basis=basis, x_min=X_min, x_max=X_max\n",
    "                    )\n",
    "                    if ucb_deriv:\n",
    "                        Psi_x_J_deriv_eval = prodspline(\n",
    "                            x=X, xeval=X_eval if X_eval is not None else None,\n",
    "                            K=K_XJ, knots=knots, basis=basis,\n",
    "                            deriv_index=deriv_index, deriv=deriv_order,\n",
    "                            x_min=X_min, x_max=X_max\n",
    "                        )\n",
    "                    if basis != \"tensor\":\n",
    "                        Psi_x_J = np.column_stack([np.ones((Psi_x_J.shape[0], 1)), Psi_x_J])\n",
    "                        Psi_x_J_eval = np.column_stack([np.ones((Psi_x_J_eval.shape[0], 1)), Psi_x_J_eval])\n",
    "                        if ucb_deriv:\n",
    "                            Psi_x_J_deriv_eval = np.column_stack([np.zeros((Psi_x_J_deriv_eval.shape[0], 1)), Psi_x_J_deriv_eval])\n",
    "\n",
    "                # Recompute tmp_J & residuals at this J\n",
    "                BtB_inv_J = _ginv(B_w_J.T @ B_w_J)\n",
    "                P_B_J = B_w_J @ BtB_inv_J @ B_w_J.T\n",
    "                PsiTP_J = Psi_x_J.T @ P_B_J\n",
    "                tmp_J = _ginv(PsiTP_J @ Psi_x_J) @ PsiTP_J\n",
    "                beta_J = tmp_J @ Y\n",
    "                U_J = Y - (Psi_x_J @ beta_J)\n",
    "\n",
    "                # Asy SEs for norming\n",
    "                Wgt_J = (tmp_J.T * U_J.ravel()[:, None])\n",
    "                D_J = Wgt_J.T @ Wgt_J\n",
    "                if ucb_h:\n",
    "                    asy_se_J = np.sqrt(np.abs(np.sum((Psi_x_J_eval @ D_J) * Psi_x_J_eval, axis=1)))\n",
    "                if ucb_deriv:\n",
    "                    asy_se_J_deriv = np.sqrt(np.abs(np.sum((Psi_x_J_deriv_eval @ D_J) * Psi_x_J_deriv_eval, axis=1)))\n",
    "\n",
    "                # Bootstrap sup t at this J (reusing Zmat rows)\n",
    "                for b in range(boot_num):\n",
    "                    z = Zmat[b]\n",
    "                    if ucb_h:\n",
    "                        num = Psi_x_J_eval @ (tmp_J @ (U_J.ravel() * z).reshape(-1, 1))\n",
    "                        boot_Z_h[b, ii] = np.max(np.abs(num.ravel() / _nzd(asy_se_J)))\n",
    "                    if ucb_deriv:\n",
    "                        num_d = Psi_x_J_deriv_eval @ (tmp_J @ (U_J.ravel() * z).reshape(-1, 1))\n",
    "                        boot_Z_d[b, ii] = np.max(np.abs(num_d.ravel() / _nzd(asy_se_J_deriv)))\n",
    "\n",
    "            # Critical values (max over J grid, then quantile, then inflate)\n",
    "            if ucb_h:\n",
    "                Z_boot = np.nanmax(boot_Z_h, axis=1)\n",
    "                z_star = float(np.quantile(Z_boot, 1 - alpha, method=\"linear\"))\n",
    "                cv = z_star + max(0.0, np.log(np.log(float(test1[\"J.tilde\"])))) * float(test1[\"theta.star\"])\n",
    "            if ucb_deriv:\n",
    "                Z_boot_d = np.nanmax(boot_Z_d, axis=1)\n",
    "                z_star_d = float(np.quantile(Z_boot_d, 1 - alpha, method=\"linear\"))\n",
    "                cv_deriv = z_star_d + max(0.0, np.log(np.log(float(test1[\"J.tilde\"])))) * float(test1[\"theta.star\"])\n",
    "\n",
    "            # restore selected J/K\n",
    "            J_x_segments = int(test1[\"J.x.seg\"])\n",
    "            K_w_segments = int(test1[\"K.w.seg\"])\n",
    "\n",
    "        else:\n",
    "            # Fixed J/K: Chen & Christensen (2018)\n",
    "            Z_h = np.empty(boot_num) if ucb_h else None\n",
    "            Z_d = np.empty(boot_num) if ucb_deriv else None\n",
    "            for b in range(boot_num):\n",
    "                z = rng.standard_normal(X.shape[0])\n",
    "                if ucb_h:\n",
    "                    num = Psi_x_eval @ (tmp @ (U_hat.ravel() * z).reshape(-1, 1))\n",
    "                    Z_h[b] = np.max(np.abs(num.ravel() / _nzd(asy_se)))\n",
    "                if ucb_deriv:\n",
    "                    num_d = Psi_x_deriv_eval @ (tmp @ (U_hat.ravel() * z).reshape(-1, 1))\n",
    "                    Z_d[b] = np.max(np.abs(num_d.ravel() / _nzd(asy_se_deriv)))\n",
    "            if ucb_h: cv = float(np.quantile(Z_h, 1 - alpha, method=\"linear\"))\n",
    "            if ucb_deriv: cv_deriv = float(np.quantile(Z_d, 1 - alpha, method=\"linear\"))\n",
    "\n",
    "        # Apply UCBs\n",
    "        if ucb_h:\n",
    "            asy_se_col = asy_se.reshape(-1, 1)\n",
    "            h_lower = h - cv * asy_se_col\n",
    "            h_upper = h + cv * asy_se_col\n",
    "        if ucb_deriv:\n",
    "            asy_se_d_col = asy_se_deriv.reshape(-1, 1)\n",
    "            h_lower_deriv = h_deriv - cv_deriv * asy_se_d_col\n",
    "            h_upper_deriv = h_deriv + cv_deriv * asy_se_d_col\n",
    "    # end UCB block\n",
    "\n",
    "    # -------- pack & return (R-style names) --------\n",
    "    return {\n",
    "        \"J.x.degree\": J_x_degree,\n",
    "        \"J.x.segments\": J_x_segments,\n",
    "        \"K.w.degree\": K_w_degree,\n",
    "        \"K.w.segments\": K_w_segments,\n",
    "        \"asy.se\": asy_se,\n",
    "        \"beta\": beta,\n",
    "        \"cv.deriv\": (None if not ucb_deriv else cv_deriv),\n",
    "        \"cv\": (None if not ucb_h else cv),\n",
    "        \"deriv.asy.se\": asy_se_deriv,\n",
    "        \"deriv.index\": deriv_index,\n",
    "        \"deriv.order\": deriv_order,\n",
    "        \"deriv\": h_deriv,\n",
    "        \"h.lower.deriv\": h_lower_deriv,\n",
    "        \"h.lower\": h_lower,\n",
    "        \"h.upper.deriv\": h_upper_deriv,\n",
    "        \"h.upper\": h_upper,\n",
    "        \"h\": h,\n",
    "        \"nevalobs\": (None if X_eval is None else X_eval.shape[0]),\n",
    "        \"nobs\": X.shape[0],\n",
    "        \"ndim\": X.shape[1],\n",
    "        \"residuals\": (Y - (Psi_x @ beta)).ravel(),\n",
    "        \"trainiseval\": (X_eval is None),\n",
    "        \"Y\": Y.ravel(),\n",
    "        \"X\": X,\n",
    "        \"X.eval\": (X if X_eval is None else X_eval),\n",
    "        \"W\": W,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(\"Engel95\", package = \"npiv\")\n",
    "Engel95 <- Engel95[order(Engel95$logexp),] \n",
    "attach(Engel95)\n",
    "logexp.eval <- seq(4.5,6.5,length=100)\n",
    "\n",
    "\n",
    "food_engel <- npiv(food, logexp, logwages, X.eval = logexp.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8942d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Python port of Jeffrey S. Racine's glp.model.matrix (R, 2011)\n",
    "# Author: ChatGPT — direct translation with NumPy\n",
    "#\n",
    "# Notes:\n",
    "# - X must be a list of 2D NumPy arrays (n x d_j), one per variable.\n",
    "# - Each X[j] must have its first column equal to 1 (intercept / degree 0).\n",
    "# - This reproduces the R logic, including the construction of the 'z' set\n",
    "#   of multi-indices and the generalized tensor product.\n",
    "# - The original R code calls a compiled C routine; here we compute the\n",
    "#   same result in pure NumPy.\n",
    "#\n",
    "# Dependencies: numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _index_swap(index_input: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Invert a permutation (R's 'index.swap').\n",
    "    index_input: permutation indices (0-based) describing sort order.\n",
    "    Returns an inverse permutation such that out[index_input[i]] = i.\n",
    "    \"\"\"\n",
    "    index_input = np.asarray(index_input, dtype=int)\n",
    "    inv = np.empty_like(index_input)\n",
    "    inv[index_input] = np.arange(index_input.size, dtype=int)\n",
    "    return inv\n",
    "\n",
    "\n",
    "def _two_dimen(d1, d2, d2p, nd1, pd12, d1sets):\n",
    "    \"\"\"\n",
    "    Port of the R helper two.dimen (uses 1-based logic for nd1-like arrays).\n",
    "    Conventions:\n",
    "      - d1, d2 are scalars (ncol of the first and second blocks).\n",
    "      - nd1 is a 1-based-like vector of length d1 (index 1..d1 meaningful).\n",
    "      - d1sets is an integer matrix whose rows are index 'sets' built so far.\n",
    "      - Some entries of the sets can be 0 (intercept for that dimension).\n",
    "      - Returns dict with keys: d12, nd1 (updated), d2p, sets.\n",
    "    \"\"\"\n",
    "    # Ensure nd1 is a Python list/array with 1-based semantics for readability.\n",
    "    nd1 = np.asarray(nd1, dtype=int)\n",
    "\n",
    "    if d2 == 1:\n",
    "        ret = {\n",
    "            \"d12\": pd12,\n",
    "            \"nd1\": nd1.copy(),\n",
    "            \"d2p\": d2,\n",
    "            \"sets\": np.hstack([d1sets, np.zeros((d1sets.shape[0], 1), dtype=int)]),\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    # Start building the combined sets\n",
    "    d12 = d2\n",
    "    rows0 = min(d1sets.shape[0], d2)\n",
    "    # Initial block: zeros for previous dims, and 1..d2 for the new dim (R used 1:d2 here)\n",
    "    right_col = np.arange(1, d2 + 1, dtype=int).reshape(-1, 1)[:rows0]\n",
    "    d2sets = np.hstack([np.zeros((rows0, d1sets.shape[1]), dtype=int), right_col])\n",
    "\n",
    "    # If d1 > d2, add blocks\n",
    "    if d1 - d2 > 0:\n",
    "        for i in range(1, d1 - d2 + 1):  # i = 1 .. (d1 - d2)\n",
    "            d12 += d2 * nd1[i - 1]  # nd1 is 1-based in original R; shift by -1\n",
    "            oneSet = d1sets\n",
    "            if d1sets.shape[1] > 1 and d2p > 0:\n",
    "                oneSet = oneSet[oneSet[:, -1] != d2p, :]\n",
    "\n",
    "            sel = oneSet[np.sum(oneSet, axis=1) == i, :]\n",
    "            if sel.size == 0 or nd1[i - 1] == 0:\n",
    "                continue\n",
    "\n",
    "            # Replicate each selected row 'd2' times\n",
    "            rep_rows = np.repeat(sel, repeats=d2, axis=0)\n",
    "            # Append 0:(d2-1) with each value repeated nd1[i]\n",
    "            last_col = np.repeat(np.arange(0, d2, dtype=int), nd1[i - 1]).reshape(-1, 1)\n",
    "            d2sets = np.vstack([d2sets, np.hstack([rep_rows, last_col])])\n",
    "\n",
    "    # Final blocks for i = 1..d2\n",
    "    for i in range(1, d2 + 1):\n",
    "        d12 += i * nd1[d1 - i]  # nd1[d1 - i] (since nd1 is 1-based in R)\n",
    "        if nd1[d1 - i] > 0:\n",
    "            oneSet = d1sets\n",
    "            if d1sets.shape[1] > 1 and d2p > 0:\n",
    "                oneSet = oneSet[oneSet[:, -1] != d2p, :]\n",
    "\n",
    "            sel = oneSet[np.sum(oneSet, axis=1) == (d1 - i + 1), :]\n",
    "            if sel.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Replicate each selected row 'i' times\n",
    "            rep_rows = np.repeat(sel, repeats=i, axis=0)\n",
    "            # Append 0:(i-1), each repeated nd1[d1-i]\n",
    "            last_col = np.repeat(np.arange(0, i, dtype=int), nd1[d1 - i]).reshape(-1, 1)\n",
    "            d2sets = np.vstack([d2sets, np.hstack([rep_rows, last_col])])\n",
    "\n",
    "    # Update nd: compute nd2 from nd1\n",
    "    nd2 = nd1.copy()\n",
    "    if d1 > 1:\n",
    "        for j in range(1, d1):  # j = 1..(d1-1) in R\n",
    "            acc = 0\n",
    "            i_lo = max(0, j - d2 + 1)  # R uses max(0, j - d2 + 1)\n",
    "            # sum nd1[i] for i = j..(max(0,...))\n",
    "            # In R this loop is ascending i=j:max(0, j-d2+1)\n",
    "            # Implement ascending with special handling of i==0 term (=1)\n",
    "            for i in range(j, i_lo - 1, -1):  # descending gives same sum; we just add all\n",
    "                if i > 0:\n",
    "                    acc += nd1[i]\n",
    "                else:\n",
    "                    acc += 1  # nd1[0] is treated as 1 in the R code\n",
    "            nd2[j] = acc\n",
    "\n",
    "    if d2 > 1:\n",
    "        # nd2[d1] (last) starts at nd1[d1] and add nd1[(d1-d2+1):(d1-1)]\n",
    "        acc = nd1[d1]\n",
    "        for i in range(d1 - d2 + 1, d1):\n",
    "            acc += nd1[i]\n",
    "        nd2[d1] = acc\n",
    "    else:\n",
    "        nd2[d1] = nd1[d1]\n",
    "\n",
    "    return {\"d12\": d12, \"nd1\": nd2, \"d2p\": d2, \"sets\": d2sets}\n",
    "\n",
    "\n",
    "def _construct_tensor_prod(dimen_input):\n",
    "    \"\"\"\n",
    "    Port of R's construct.tensor.prod.\n",
    "    Returns the 'sets' matrix (called 'z' later) with columns aligned\n",
    "    to the original dimension order.\n",
    "    \"\"\"\n",
    "    dimen_input = np.asarray(dimen_input, dtype=int)\n",
    "    order_desc = np.argsort(dimen_input)[::-1]  # indices sorting by decreasing dim\n",
    "    dimen = dimen_input[order_desc]\n",
    "    k = dimen.size\n",
    "\n",
    "    # nd1: length dimen[0] (R is 1-based; we keep an extra slot at front for clarity)\n",
    "    # We'll store nd1 as length (d1 + 1), where index 1..d1 are active.\n",
    "    d1 = int(dimen[0])\n",
    "    nd1 = np.ones(d1 + 1, dtype=int)\n",
    "    nd1[d1] = 0  # R: nd1[d1] <- 0\n",
    "\n",
    "    ncol_bs = d1\n",
    "    sets = np.arange(1, d1 + 1, dtype=int).reshape(-1, 1)  # R: 1:d1 as a column\n",
    "    d2p = 0\n",
    "\n",
    "    if k > 1:\n",
    "        for i in range(1, k):\n",
    "            out = _two_dimen(d1, int(dimen[i]), d2p, nd1[1:], ncol_bs, sets)\n",
    "            nd1 = np.concatenate([[0], out[\"nd1\"]])  # keep 1-based convention\n",
    "            ncol_bs = out[\"d12\"]\n",
    "            sets = out[\"sets\"]\n",
    "            d2p = out[\"d2p\"]\n",
    "\n",
    "        # R: ncol.bs <- dim.rt$d12 + k - 1; then append k-1 rows (pure terms)\n",
    "        ncol_bs = out[\"d12\"] + k - 1\n",
    "        for i in range(1, k):\n",
    "            one_row = np.zeros((1, sets.shape[1]), dtype=int)\n",
    "            one_row[0, i - 1] = dimen[i - 1]\n",
    "            sets = np.vstack([sets, one_row])\n",
    "\n",
    "    # Reorder columns back to the original variable order\n",
    "    inv = _index_swap(order_desc)\n",
    "    sets = sets[:, inv]\n",
    "    return sets\n",
    "\n",
    "\n",
    "def glp_model_matrix(X):\n",
    "    \"\"\"\n",
    "    Generalized Local Polynomial model matrix (Python/NumPy port).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : list of np.ndarray\n",
    "        List of model matrices [X1, X2, ..., Xk], each of shape (n, d_j).\n",
    "        The first column of each Xj must be ones (intercept).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    B : np.ndarray\n",
    "        The generalized polynomial model matrix of shape (n, q),\n",
    "        where q is determined by the algorithm (typically much\n",
    "        smaller than the full tensor-product dimension).\n",
    "    \"\"\"\n",
    "    if not isinstance(X, (list, tuple)) or len(X) == 0:\n",
    "        raise ValueError(\"X must be a non-empty list of 2D numpy arrays.\")\n",
    "\n",
    "    # Validate shapes and get dimensions\n",
    "    n = X[0].shape[0]\n",
    "    for i, Xi in enumerate(X):\n",
    "        if Xi.ndim != 2:\n",
    "            raise ValueError(f\"X[{i}] must be 2D.\")\n",
    "        if Xi.shape[0] != n:\n",
    "            raise ValueError(\"All X[j] must have the same number of rows.\")\n",
    "\n",
    "    d = np.array([Xi.shape[1] for Xi in X], dtype=int)\n",
    "    k = len(X)\n",
    "\n",
    "    # Build 'z' (called 'sets' in the helpers).\n",
    "    z = _construct_tensor_prod(d)\n",
    "\n",
    "    # Match R's reordering to expand.grid: order rows by last->first columns\n",
    "    # (R: z <- z[do.call('order', as.list(data.frame(z[, NCOL(z):1]))), ])\n",
    "    if z.shape[1] > 1:\n",
    "        # np.lexsort uses last key as primary; give it columns from first to last,\n",
    "        # so we reverse to get last-to-first primary ordering.\n",
    "        sort_keys = tuple(z[:, j] for j in range(z.shape[1] - 1, -1, -1))\n",
    "        ord_rows = np.lexsort(sort_keys)\n",
    "        z = z[ord_rows, :]\n",
    "\n",
    "    # Build the result: for each row r of z, multiply the selected columns across variables.\n",
    "    # Map set value v -> column index:\n",
    "    #   v == 0  -> use column 0  (intercept)\n",
    "    #   v >= 1  -> use column (v - 1)\n",
    "    n_terms = z.shape[0]\n",
    "    B = np.ones((n, n_terms), dtype=float)\n",
    "\n",
    "    # Precompute per-variable column blocks selected by z\n",
    "    for j in range(k):\n",
    "        zj = z[:, j]\n",
    "        cols_j = np.where(zj <= 0, 0, zj - 1)  # vector of column indices for X[j]\n",
    "        # Gather columns for all terms at once: shape (n, n_terms)\n",
    "        block = X[j][:, cols_j]\n",
    "        B *= block\n",
    "\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bc4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B shape: (1000, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\187697\\AppData\\Local\\Temp\\ipykernel_32004\\3777207578.py:242: RuntimeWarning: some 'x' values beyond boundary knots may cause ill-conditioned bases\n",
      "  warnings.warn(\"some 'x' values beyond boundary knots may cause ill-conditioned bases\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'math'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 348\u001b[0m\n\u001b[0;32m    346\u001b[0m B_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit_transform(x)\n\u001b[0;32m    347\u001b[0m x_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m200\u001b[39m)  \u001b[38;5;66;03m# includes outside points\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m B_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB_train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, B_train\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB_new:\u001b[39m\u001b[38;5;124m\"\u001b[39m, B_new\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[2], line 318\u001b[0m, in \u001b[0;36mGslBS.transform\u001b[1;34m(self, newx)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, newx: Union[np\u001b[38;5;241m.\u001b[39mndarray, Iterable[\u001b[38;5;28mfloat\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# Predict using stored attributes (like predict.gsl.bs)\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     B, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgsl_bs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnbreak\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbreak\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknots\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknots\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mderiv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderiv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m B\n",
      "Cell \u001b[1;32mIn[2], line 262\u001b[0m, in \u001b[0;36mgsl_bs\u001b[1;34m(x, degree, nbreak, deriv, x_min, x_max, intercept, knots)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(ol) \u001b[38;5;129;01mand\u001b[39;00m (degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m deriv):\n\u001b[0;32m    261\u001b[0m     pivot \u001b[38;5;241m=\u001b[39m breaks[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# left boundary\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     B[ol, :] \u001b[38;5;241m=\u001b[39m \u001b[43m_taylor_extrapolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpivot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderiv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Outside-right: Taylor around pivot = x_max (or max(knots) if provided)\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(or_) \u001b[38;5;129;01mand\u001b[39;00m (degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m deriv):\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36m_taylor_extrapolate\u001b[1;34m(x_out, pivot, t, degree, deriv)\u001b[0m\n\u001b[0;32m     94\u001b[0m T \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([_bspline_design_matrix(np\u001b[38;5;241m.\u001b[39marray([pivot], \u001b[38;5;28mfloat\u001b[39m), t, degree, s)[\u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m orders])\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Divide row r by (r-1)! (gamma(r)) so rows correspond to f^{(deriv+q)}(pivot) / q!\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# where q = 0..(L-1), and gamma(q+1) = q!\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m gamma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mfactorial(q) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L)], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     99\u001b[0m T_scaled \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m/\u001b[39m gamma[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Build polynomial features: [1, (x-pivot), (x-pivot)^2, ..., (x-pivot)^(L-1)]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     94\u001b[0m T \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([_bspline_design_matrix(np\u001b[38;5;241m.\u001b[39marray([pivot], \u001b[38;5;28mfloat\u001b[39m), t, degree, s)[\u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m orders])\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Divide row r by (r-1)! (gamma(r)) so rows correspond to f^{(deriv+q)}(pivot) / q!\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# where q = 0..(L-1), and gamma(q+1) = q!\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m gamma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241m.\u001b[39mfactorial(q) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L)], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     99\u001b[0m T_scaled \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m/\u001b[39m gamma[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Build polynomial features: [1, (x-pivot), (x-pivot)^2, ..., (x-pivot)^(L-1)]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\187697\\Dropbox\\India_forest_land\\C_Programs\\sd_new_data\\forest_land\\lib\\site-packages\\numpy\\__init__.py:414\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'math'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "\n",
    "# ---------- internal helpers ----------\n",
    "\n",
    "def _make_breaks(nbreak: int, x_min: float, x_max: float, knots: Optional[Iterable[float]]):\n",
    "    \"\"\"\n",
    "    Returns the 'breakpoints' vector of length nbreak (including the two boundaries).\n",
    "    If `knots` is provided, it must be length nbreak and sorted (R code enforces len == nbreak).\n",
    "    \"\"\"\n",
    "    if knots is not None:\n",
    "        b = np.asarray(knots, dtype=float)\n",
    "        if b.size != nbreak:\n",
    "            raise ValueError(\"`knots` length must equal `nbreak`.\")\n",
    "        if np.any(np.diff(b) < 0):\n",
    "            b = np.sort(b)\n",
    "        return b\n",
    "    # equally spaced breakpoints, including endpoints\n",
    "    return np.linspace(x_min, x_max, nbreak)\n",
    "\n",
    "\n",
    "def _augmented_knots(breaks: np.ndarray, degree: int):\n",
    "    \"\"\"\n",
    "    Construct the open knot vector with endpoint multiplicity p+1 and interior breaks with multiplicity 1.\n",
    "    \"\"\"\n",
    "    p = int(degree)\n",
    "    if breaks.ndim != 1 or breaks.size < 2:\n",
    "        raise ValueError(\"breaks must be a 1D array with at least two values.\")\n",
    "    left = np.repeat(breaks[0], p + 1)\n",
    "    right = np.repeat(breaks[-1], p + 1)\n",
    "    interior = breaks[1:-1]\n",
    "    t = np.concatenate([left, interior, right]).astype(float)\n",
    "    # n_basis = len(t) - p - 1  == nbreak + degree - 1\n",
    "    return t\n",
    "\n",
    "\n",
    "def _bspline_design_matrix(x: np.ndarray, t: np.ndarray, degree: int, deriv_order: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate all B-spline basis functions (or their `deriv_order` derivatives) at x.\n",
    "    Returns matrix shape (len(x), n_basis).\n",
    "    \"\"\"\n",
    "    p = int(degree)\n",
    "    n_basis = len(t) - p - 1\n",
    "    if n_basis <= 0:\n",
    "        raise ValueError(\"Invalid knot configuration -> non-positive number of basis functions.\")\n",
    "\n",
    "    # Build basis splines with identity coefficients\n",
    "    # (one BSpline per column; keep extrapolate=False here; caller decides how to handle outside)\n",
    "    eye = np.eye(n_basis)\n",
    "    col_spl = []\n",
    "    for i in range(n_basis):\n",
    "        spl = BSpline(t, eye[i], p, extrapolate=False)\n",
    "        if deriv_order > 0:\n",
    "            spl = spl.derivative(deriv_order)\n",
    "        col_spl.append(spl)\n",
    "\n",
    "    # Evaluate\n",
    "    X = np.empty((x.size, n_basis), dtype=float)\n",
    "    for j, spl in enumerate(col_spl):\n",
    "        X[:, j] = spl(x)\n",
    "    return X\n",
    "\n",
    "\n",
    "def _taylor_extrapolate(\n",
    "    x_out: np.ndarray,\n",
    "    pivot: float,\n",
    "    t: np.ndarray,\n",
    "    degree: int,\n",
    "    deriv: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For points outside support, reproduce the R code's Taylor expansion around the boundary pivot.\n",
    "    Returns a matrix with shape (len(x_out), n_basis) corresponding to the `deriv`-th derivative values.\n",
    "    \"\"\"\n",
    "    p = int(degree)\n",
    "    n_basis = len(t) - p - 1\n",
    "\n",
    "    if deriv > p + 1:  # R: deriv <= degree + 1 checked earlier; this case would be zero anyway\n",
    "        return np.zeros((x_out.size, n_basis), dtype=float)\n",
    "\n",
    "    # Number of terms in the expansion: q = 0..(p-deriv)\n",
    "    L = p - deriv + 1\n",
    "    if L <= 0:\n",
    "        # deriv == degree+1 => everything is zero (outside and inside)\n",
    "        return np.zeros((x_out.size, n_basis), dtype=float)\n",
    "\n",
    "    # Build T: rows correspond to orders `deriv, deriv+1, ..., degree` evaluated at pivot\n",
    "    orders = list(range(deriv, p + 1))\n",
    "    T = np.vstack([_bspline_design_matrix(np.array([pivot], float), t, degree, s)[0, :] for s in orders])\n",
    "\n",
    "    # Divide row r by (r-1)! (gamma(r)) so rows correspond to f^{(deriv+q)}(pivot) / q!\n",
    "    # where q = 0..(L-1), and gamma(q+1) = q!\n",
    "    gamma = np.array([np.math.factorial(q) for q in range(L)], dtype=float)\n",
    "    T_scaled = T / gamma[:, None]\n",
    "\n",
    "    # Build polynomial features: [1, (x-pivot), (x-pivot)^2, ..., (x-pivot)^(L-1)]\n",
    "    dx = (x_out - pivot).reshape(-1, 1)\n",
    "    # (broadcasted) powers per column q\n",
    "    poly = np.hstack([np.ones_like(dx)] + [dx ** q for q in range(1, L)])\n",
    "\n",
    "    return poly @ T_scaled  # shape (#x_out, n_basis)\n",
    "\n",
    "\n",
    "# ---------- public API (Python port) ----------\n",
    "\n",
    "def bs_des(\n",
    "    x: Union[np.ndarray, Iterable[float]],\n",
    "    degree: int = 3,\n",
    "    nbreak: int = 2,\n",
    "    deriv: Union[int, Iterable[int]] = 0,\n",
    "    x_min: Optional[float] = None,\n",
    "    x_max: Optional[float] = None,\n",
    "    knots: Optional[Iterable[float]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Python port of R's bs.des (basis/derivative evaluator).\n",
    "    - x: points to evaluate (1D array)\n",
    "    - degree: spline degree p >= 1\n",
    "    - nbreak: number of breakpoints (including the two boundaries)\n",
    "    - deriv: either a scalar derivative order or a vector of per-x derivative orders\n",
    "    - x_min, x_max: boundaries (used when `knots` is None)\n",
    "    - knots: optional breakpoints (length must equal nbreak)\n",
    "\n",
    "    Returns: (n, nbreak + degree - 1) matrix\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    n = x.size\n",
    "\n",
    "    if degree <= 0:\n",
    "        raise ValueError(\"degree must be a positive integer\")\n",
    "    if nbreak <= 1:\n",
    "        raise ValueError(\"nbreak must be at least 2\")\n",
    "\n",
    "    # bounds\n",
    "    if x_min is None:\n",
    "        x_min = float(np.min(x))\n",
    "    if x_max is None:\n",
    "        x_max = float(np.max(x))\n",
    "    if x_min >= x_max:\n",
    "        raise ValueError(\"x_min must be less than x_max\")\n",
    "\n",
    "    # breaks and augmented knot vector\n",
    "    breaks = _make_breaks(nbreak, x_min, x_max, knots)\n",
    "    t = _augmented_knots(breaks, degree)\n",
    "\n",
    "    # deriv vector per x\n",
    "    if np.isscalar(deriv):\n",
    "        deriv_vec = np.full(n, int(deriv), dtype=int)\n",
    "    else:\n",
    "        deriv_vec = np.asarray(deriv, dtype=int).ravel()\n",
    "        if deriv_vec.size != n:\n",
    "            raise ValueError(\"`deriv` must be a scalar or a vector with the same length as `x`.\")\n",
    "\n",
    "    if np.any(deriv_vec < 0):\n",
    "        raise ValueError(\"deriv must be non-negative\")\n",
    "    if np.any(deriv_vec > degree + 1):\n",
    "        raise ValueError(\"deriv must be <= degree + 1\")\n",
    "\n",
    "    # Evaluate by grouping unique derivative orders for efficiency\n",
    "    B = np.empty((n, len(t) - degree - 1), dtype=float)\n",
    "    for d_ord in np.unique(deriv_vec):\n",
    "        idx = np.where(deriv_vec == d_ord)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        B[idx, :] = _bspline_design_matrix(x[idx], t, degree, int(d_ord))\n",
    "\n",
    "    return B\n",
    "\n",
    "\n",
    "def gsl_bs(\n",
    "    x: Union[np.ndarray, Iterable[float]],\n",
    "    degree: int = 3,\n",
    "    nbreak: int = 2,\n",
    "    deriv: int = 0,\n",
    "    x_min: Optional[float] = None,\n",
    "    x_max: Optional[float] = None,\n",
    "    intercept: bool = False,\n",
    "    knots: Optional[Iterable[float]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Python port of R's gsl.bs.default.\n",
    "    Returns the generalized spline basis (or derivative basis) matrix.\n",
    "\n",
    "    Notes:\n",
    "    - If `x` lies outside the boundary knots, we reproduce the R code's\n",
    "      Taylor expansion using derivatives at the boundary to avoid ill-conditioning.\n",
    "    - If `intercept=False` (R's default), the first column is dropped.\n",
    "    - Attributes from the R object are returned alongside the matrix.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "\n",
    "    if degree <= 0:\n",
    "        raise ValueError(\"degree must be a positive integer\")\n",
    "    if deriv < 0:\n",
    "        raise ValueError(\"deriv must be non-negative\")\n",
    "    if deriv > degree + 1:\n",
    "        raise ValueError(\"deriv must be <= degree + 1\")\n",
    "    if nbreak <= 1:\n",
    "        raise ValueError(\"nbreak must be at least 2\")\n",
    "\n",
    "    # reconcile nbreak with provided knots (R resets nbreak and warns)\n",
    "    if knots is not None:\n",
    "        knots = np.asarray(knots, dtype=float).ravel()\n",
    "        if knots.size != nbreak:\n",
    "            nbreak = int(knots.size)\n",
    "            warnings.warn(f\"nbreak and knots vector do not agree: resetting nbreak to {nbreak}\", RuntimeWarning)\n",
    "\n",
    "    # bounds: when knots are provided, outside check uses min/max(knots)\n",
    "    # otherwise, x_min/x_max come from data if not provided\n",
    "    if x_min is None:\n",
    "        x_min = float(np.min(x))\n",
    "        ol = np.zeros_like(x, dtype=bool)\n",
    "    else:\n",
    "        ol = x < x_min\n",
    "\n",
    "    if x_max is None:\n",
    "        x_max = float(np.max(x))\n",
    "        or_ = np.zeros_like(x, dtype=bool)\n",
    "    else:\n",
    "        or_ = x > x_max\n",
    "\n",
    "    if knots is not None:\n",
    "        kmin, kmax = float(np.min(knots)), float(np.max(knots))\n",
    "        ol = ol | (x < kmin)\n",
    "        or_ = or_ | (x > kmax)\n",
    "\n",
    "    outside = ol | or_\n",
    "    inside = ~outside\n",
    "\n",
    "    # Build breaks & augmented knots once\n",
    "    breaks = _make_breaks(nbreak, x_min, x_max, knots)\n",
    "    t = _augmented_knots(breaks, degree)\n",
    "    ncol = len(t) - degree - 1\n",
    "\n",
    "    # Evaluate inside points directly\n",
    "    if np.any(outside):\n",
    "        warnings.warn(\"some 'x' values beyond boundary knots may cause ill-conditioned bases\", RuntimeWarning)\n",
    "\n",
    "    # Initialize result\n",
    "    B = np.zeros((x.size, ncol), dtype=float)\n",
    "\n",
    "    # Inside points: evaluate derivative `deriv` normally\n",
    "    if np.any(inside):\n",
    "        B[inside, :] = bs_des(\n",
    "            x[inside],\n",
    "            degree=degree,\n",
    "            nbreak=nbreak,\n",
    "            deriv=int(deriv),\n",
    "            x_min=x_min,\n",
    "            x_max=x_max,\n",
    "            knots=knots,\n",
    "        )\n",
    "\n",
    "    # Outside-left: Taylor around pivot = x_min (or min(knots) if provided)\n",
    "    if np.any(ol) and (degree + 1 > deriv):\n",
    "        pivot = breaks[0]  # left boundary\n",
    "        B[ol, :] = _taylor_extrapolate(x[ol], pivot, t, degree, deriv)\n",
    "\n",
    "    # Outside-right: Taylor around pivot = x_max (or max(knots) if provided)\n",
    "    if np.any(or_) and (degree + 1 > deriv):\n",
    "        pivot = breaks[-1]  # right boundary\n",
    "        B[or_, :] = _taylor_extrapolate(x[or_], pivot, t, degree, deriv)\n",
    "\n",
    "    # Drop the first column if intercept == False (R default)\n",
    "    if not intercept:\n",
    "        B = B[:, 1:].copy()\n",
    "\n",
    "    # Return matrix + attributes for parity with R object\n",
    "    attrs = {\n",
    "        \"degree\": degree,\n",
    "        \"nbreak\": nbreak,\n",
    "        \"deriv\": deriv,\n",
    "        \"x.min\": x_min,\n",
    "        \"x.max\": x_max,\n",
    "        \"intercept\": intercept,\n",
    "        \"knots\": None if knots is None else np.asarray(knots, dtype=float),\n",
    "    }\n",
    "    return B, attrs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GslBS:\n",
    "    \"\"\"\n",
    "    Small convenience wrapper mimicking R's object + predict method.\n",
    "    \"\"\"\n",
    "    degree: int = 3\n",
    "    nbreak: int = 2\n",
    "    deriv: int = 0\n",
    "    intercept: bool = False\n",
    "    knots: Optional[np.ndarray] = None\n",
    "    x_min: Optional[float] = None\n",
    "    x_max: Optional[float] = None\n",
    "\n",
    "    def fit_transform(self, x: Union[np.ndarray, Iterable[float]]) -> np.ndarray:\n",
    "        B, attrs = gsl_bs(\n",
    "            x,\n",
    "            degree=self.degree,\n",
    "            nbreak=self.nbreak,\n",
    "            deriv=self.deriv,\n",
    "            x_min=self.x_min,\n",
    "            x_max=self.x_max,\n",
    "            intercept=self.intercept,\n",
    "            knots=self.knots,\n",
    "        )\n",
    "        # persist attributes as in R\n",
    "        self.x_min = attrs[\"x.min\"]\n",
    "        self.x_max = attrs[\"x.max\"]\n",
    "        self.knots = attrs[\"knots\"]\n",
    "        return B\n",
    "\n",
    "    def transform(self, newx: Union[np.ndarray, Iterable[float]]) -> np.ndarray:\n",
    "        # Predict using stored attributes (like predict.gsl.bs)\n",
    "        B, _ = gsl_bs(\n",
    "            newx,\n",
    "            degree=self.degree,\n",
    "            nbreak=self.nbreak if self.knots is None else len(self.knots),\n",
    "            deriv=self.deriv,\n",
    "            x_min=self.x_min,\n",
    "            x_max=self.x_max,\n",
    "            intercept=self.intercept,\n",
    "            knots=self.knots,\n",
    "        )\n",
    "        return B\n",
    "\n",
    "\n",
    "# ---------- quick example ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rng = np.random.default_rng(42)\n",
    "    n = 1000\n",
    "    degree = 3\n",
    "    nbreak = 5\n",
    "\n",
    "    x = rng.random(n)  # U[0,1]\n",
    "    # Fit + transform\n",
    "    B, attrs = gsl_bs(x, degree=degree, nbreak=nbreak)  # default deriv=0, intercept=False\n",
    "    print(\"B shape:\", B.shape)  # (n, nbreak + degree - 1 - 1) because intercept=False\n",
    "\n",
    "    # Predict on new data using the class\n",
    "    model = GslBS(degree=degree, nbreak=nbreak, intercept=False)\n",
    "    B_train = model.fit_transform(x)\n",
    "    x_new = np.linspace(-0.2, 1.2, 200)  # includes outside points\n",
    "    B_new = model.transform(x_new)\n",
    "    print(\"B_train:\", B_train.shape, \"B_new:\", B_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Optional, Iterable, Dict, Any, Callable\n",
    "\n",
    "\n",
    "def tensor_prod_model_matrix(X: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Python port of mgcv::tensor.prod.model.matrix.\n",
    "\n",
    "    Given a list of design matrices X = [X1, X2, ... , Xm] with the same number\n",
    "    of rows n and columns d_j, returns an (n x prod(d_j)) matrix whose i-th row is\n",
    "    the Kronecker product X1[i, :] ⊗ X2[i, :] ⊗ ... ⊗ Xm[i, :].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : list of 2D numpy arrays\n",
    "        Each Xi has shape (n, d_i). All must have the same n.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    T : 2D numpy array\n",
    "        Shape (n, prod(d_i)). Column order matches the iterative\n",
    "        Khatri–Rao construction (outermost loop over later Xi).\n",
    "    \"\"\"\n",
    "    if not isinstance(X, (list, tuple)) or len(X) == 0:\n",
    "        raise ValueError(\"X must be a non-empty list of 2D NumPy arrays.\")\n",
    "\n",
    "    n = X[0].shape[0]\n",
    "    for i, Xi in enumerate(X):\n",
    "        if Xi.ndim != 2:\n",
    "            raise ValueError(f\"X[{i}] must be 2D.\")\n",
    "        if Xi.shape[0] != n:\n",
    "            raise ValueError(\"All X[j] must have the same number of rows.\")\n",
    "\n",
    "    # Build via iterative Khatri–Rao (column-wise row Kronecker).\n",
    "    T = np.ones((n, 1), dtype=float)\n",
    "    for Xi in X:\n",
    "        # For each column c of Xi, multiply T element-wise by Xi[:, c] and stack\n",
    "        T = np.hstack([T * Xi[:, [c]] for c in range(Xi.shape[1])])\n",
    "\n",
    "    return T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee8917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Optional, Union, Mapping\n",
    "from patsy import dmatrix\n",
    "\n",
    "\n",
    "def _split_npiv_formula(formula: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Split 'Y ~ X | W' into lhs, rhs_x, rhs_w (strings).\n",
    "    \"\"\"\n",
    "    if \"~\" not in formula:\n",
    "        raise ValueError(\"Formula must contain '~', e.g. 'y ~ x1 + x2 | z1 + z2'.\")\n",
    "    lhs, rhs = formula.split(\"~\", 1)\n",
    "    lhs = lhs.strip()\n",
    "    if \"|\" not in rhs:\n",
    "        raise ValueError(\"Formula must contain '|' to separate X and W: 'y ~ X | W'.\")\n",
    "    rhs_x, rhs_w = rhs.split(\"|\", 1)\n",
    "    rhs_x, rhs_w = rhs_x.strip(), rhs_w.strip()\n",
    "    if not lhs or not rhs_x or not rhs_w:\n",
    "        raise ValueError(\"Parsed formula parts are empty; check your formula string.\")\n",
    "    return {\"lhs\": lhs, \"rhs_x\": rhs_x, \"rhs_w\": rhs_w}\n",
    "\n",
    "\n",
    "def _apply_subset(df: pd.DataFrame, subset: Optional[Union[str, np.ndarray, pd.Series]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a subset (string query or boolean index/array). Returns a view.\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        return df\n",
    "    if isinstance(subset, str):\n",
    "        return df.query(subset)\n",
    "    # boolean mask / indexer\n",
    "    return df.loc[subset]\n",
    "\n",
    "\n",
    "def npiv_formula(\n",
    "    formula: str,\n",
    "    data: Optional[Union[pd.DataFrame, Mapping[str, Any]]] = None,\n",
    "    newdata: Optional[Union[pd.DataFrame, Mapping[str, Any]]] = None,\n",
    "    subset: Optional[Union[str, np.ndarray, pd.Series]] = None,\n",
    "    na_action: str = \"na.omit\",  # supports \"na.omit\" (drop) or \"na.raise\"\n",
    "    *,\n",
    "    npiv_default_fn=None,  # inject your previously ported npiv_default\n",
    "    **kwargs: Any,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python port of R's `npiv.formula`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula : str\n",
    "        'Y ~ x-terms | w-terms' (use standard patsy syntax for terms).\n",
    "    data : DataFrame or mapping\n",
    "        Training data.\n",
    "    newdata : DataFrame or mapping, optional\n",
    "        If provided, builds X_eval using the X part of the formula.\n",
    "    subset : str or boolean index, optional\n",
    "        Row filter (pandas .query string or boolean index).\n",
    "    na_action : {\"na.omit\",\"na.raise\"}\n",
    "        Drop rows with missing values in any of the used variables, or raise.\n",
    "    npiv_default_fn : callable\n",
    "        The Python port of `npiv.default` you already have (named here to avoid shadowing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    est : dict\n",
    "        Result of `npiv_default_fn(...)` augmented with 'call' and 'formula'.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Intercepts on RHS are removed (like `[,-1]` in R).\n",
    "    - We align Y, X, and W on the intersection of valid (non-NA) rows.\n",
    "    \"\"\"\n",
    "    if npiv_default_fn is None:\n",
    "        raise ValueError(\"Please pass your `npiv_default` function as `npiv_default_fn=...`.\")\n",
    "\n",
    "    parts = _split_npiv_formula(formula)\n",
    "    lhs, rhs_x, rhs_w = parts[\"lhs\"], parts[\"rhs_x\"], parts[\"rhs_w\"]\n",
    "\n",
    "    if data is None:\n",
    "        data = {}\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    # subset first\n",
    "    df = _apply_subset(data, subset)\n",
    "\n",
    "    # NA handling: use patsy with NA_action\n",
    "    if na_action in (\"na.omit\", \"omit\", \"drop\"):\n",
    "        patsy_na = \"drop\"\n",
    "    elif na_action in (\"na.raise\", \"raise\"):\n",
    "        patsy_na = \"raise\"\n",
    "    else:\n",
    "        raise ValueError(\"na_action must be 'na.omit' (drop) or 'na.raise'.\")\n",
    "\n",
    "    # Build Y, X, W as DataFrames (keep indices to align)\n",
    "    # Y: evaluate expression on the RHS side of dmatrix ('0 + expr' -> no intercept)\n",
    "    y_df = dmatrix(f\"0 + {lhs}\", df, return_type=\"dataframe\", NA_action=patsy_na)\n",
    "    # X, W: drop intercept (R used [,-1]); '0 +' ensures no implicit intercept\n",
    "    X_df = dmatrix(f\"0 + {rhs_x}\", df, return_type=\"dataframe\", NA_action=patsy_na)\n",
    "    W_df = dmatrix(f\"0 + {rhs_w}\", df, return_type=\"dataframe\", NA_action=patsy_na)\n",
    "\n",
    "    # Align on common index (intersection of rows that survived NA drop per part)\n",
    "    idx = y_df.index.intersection(X_df.index).intersection(W_df.index)\n",
    "    if len(idx) == 0:\n",
    "        raise ValueError(\"No rows left after NA handling / subset alignment.\")\n",
    "\n",
    "    y = np.asarray(y_df.loc[idx]).ravel()\n",
    "    X = np.asarray(X_df.loc[idx])\n",
    "    W = np.asarray(W_df.loc[idx])\n",
    "\n",
    "    # Optional evaluation data for X\n",
    "    X_eval = None\n",
    "    if newdata is not None:\n",
    "        nd = newdata if isinstance(newdata, pd.DataFrame) else pd.DataFrame(newdata)\n",
    "        X_eval_df = dmatrix(f\"0 + {rhs_x}\", nd, return_type=\"dataframe\", NA_action=patsy_na)\n",
    "        X_eval = np.asarray(X_eval_df)\n",
    "\n",
    "    # Call your npiv_default\n",
    "    est = npiv_default_fn(\n",
    "        Y=y,\n",
    "        X=X,\n",
    "        W=W,\n",
    "        X_eval=X_eval,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # Add call & formula (parity with R)\n",
    "    est[\"call\"] = f\"npiv_formula(formula='{formula}', data=..., newdata={'provided' if newdata is not None else 'None'})\"\n",
    "    est[\"formula\"] = formula\n",
    "    est.setdefault(\"_class\", \"npiv\")\n",
    "    return est\n",
    "\n",
    "\n",
    "def fitted_npiv(obj: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Python analogue of `fitted.npiv`: returns object$h.\n",
    "    \"\"\"\n",
    "    if \"h\" not in obj:\n",
    "        raise KeyError(\"Expected key 'h' in npiv result (fitted values).\")\n",
    "    return np.asarray(obj[\"h\"])\n",
    "\n",
    "\n",
    "def residuals_npiv(obj: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Python analogue of `residuals.npiv`: returns object$residuals.\n",
    "    \"\"\"\n",
    "    if \"residuals\" not in obj:\n",
    "        raise KeyError(\"Expected key 'residuals' in npiv result.\")\n",
    "    return np.asarray(obj[\"residuals\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Iterable, Dict, Any, Callable, Sequence\n",
    "\n",
    "try:\n",
    "    from tqdm import trange\n",
    "except Exception:  # tqdm is optional\n",
    "    def trange(n, **kwargs):\n",
    "        return range(n)\n",
    "\n",
    "\n",
    "# -------- small helpers --------\n",
    "\n",
    "def _match_arg(val: str, choices: Iterable[str], name: str) -> str:\n",
    "    choices = list(choices)\n",
    "    if val not in choices:\n",
    "        raise ValueError(f\"{name} must be one of {choices}, got {val!r}.\")\n",
    "    return val\n",
    "\n",
    "def _ginv(A: np.ndarray) -> np.ndarray:\n",
    "    return np.linalg.pinv(A, rcond=1e-12)\n",
    "\n",
    "def _is_fullrank(M: np.ndarray) -> bool:\n",
    "    if M.ndim == 1:\n",
    "        M = M[:, None]\n",
    "    return np.linalg.matrix_rank(M) == M.shape[1]\n",
    "\n",
    "def _as_colvec(y) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    return y.reshape(-1, 1)\n",
    "\n",
    "def _nzd(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    return np.where(np.abs(x) < eps, eps, x)\n",
    "\n",
    "\n",
    "# -------- main function --------\n",
    "\n",
    "def npiv_J(\n",
    "    Y,\n",
    "    X: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    X_grid: Optional[np.ndarray] = None,\n",
    "    J_x_degree: int = 3,\n",
    "    K_w_degree: int = 4,\n",
    "    J_x_segments_set: Optional[Sequence[int]] = None,   # default: [1,3,7,15,31,63]\n",
    "    K_w_segments_set: Optional[Sequence[int]] = None,   # default: [1,3,7,15,31,63]\n",
    "    knots: str = \"uniform\",                             # {\"uniform\",\"quantiles\"}\n",
    "    basis: str = \"tensor\",                              # {\"tensor\",\"additive\",\"glp\"}\n",
    "    X_min: Optional[float] = None,\n",
    "    X_max: Optional[float] = None,\n",
    "    W_min: Optional[float] = None,\n",
    "    W_max: Optional[float] = None,\n",
    "    grid_num: int = 50,\n",
    "    boot_num: int = 99,\n",
    "    alpha: float = 0.5,\n",
    "    check_is_fullrank: bool = False,\n",
    "    progress: bool = True,\n",
    "    *,\n",
    "    prodspline: Callable[..., np.ndarray],\n",
    "    dimbs: Callable[..., int],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python port of R's npivJ (Lepski selection via sup-t comparisons).\n",
    "\n",
    "    You must inject:\n",
    "      - prodspline(x, K, knots, basis, x_min, x_max, xeval=None) -> ndarray\n",
    "        (return basis WITHOUT leading intercept; this function adds it when needed)\n",
    "      - dimbs(basis, degree, segments) -> int  (dimension of X basis)\n",
    "\n",
    "    Returns dict with keys: J.tilde, J.hat, J.hat.n, J.x.seg, K.w.seg, theta.star\n",
    "    \"\"\"\n",
    "    basis = _match_arg(basis, [\"tensor\", \"additive\", \"glp\"], \"basis\")\n",
    "    knots = _match_arg(knots, [\"uniform\", \"quantiles\"], \"knots\")\n",
    "\n",
    "    if Y is None: raise ValueError(\"must provide Y\")\n",
    "    if X is None: raise ValueError(\"must provide X\")\n",
    "    if W is None: raise ValueError(\"must provide W\")\n",
    "    if K_w_degree < 0: raise ValueError(\"K_w_degree must be non-negative\")\n",
    "    if J_x_degree < 0: raise ValueError(\"J_x_degree must be non-negative\")\n",
    "    if not (0 < alpha < 1): raise ValueError(\"alpha must lie in (0,1)\")\n",
    "\n",
    "    Y = _as_colvec(Y)\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    W = np.asarray(W, dtype=float)\n",
    "\n",
    "    if check_is_fullrank:\n",
    "        if not _is_fullrank(Y): raise ValueError(\"Y is not of full column rank\")\n",
    "        if not _is_fullrank(X): raise ValueError(\"X is not of full column rank\")\n",
    "        if not _is_fullrank(W): raise ValueError(\"W is not of full column rank\")\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    # If no grid given, make a simple column-wise grid (not a Cartesian product).\n",
    "    if X_grid is None:\n",
    "        X_grid = np.empty((grid_num, p))\n",
    "        for j in range(p):\n",
    "            col = X[:, j]\n",
    "            X_grid[:, j] = np.linspace(np.min(col), np.max(col), grid_num)\n",
    "\n",
    "    # Default segment sets: c(2,4,8,16,32,64) - 1  ->  [1,3,7,15,31,63]\n",
    "    if J_x_segments_set is None:\n",
    "        J_x_segments_set = (2 ** np.arange(1, 7) - 1).tolist()\n",
    "    if K_w_segments_set is None:\n",
    "        K_w_segments_set = (2 ** np.arange(1, 7) - 1).tolist()\n",
    "\n",
    "    J_x_segments_set = np.asarray(J_x_segments_set, dtype=int)\n",
    "    K_w_segments_set = np.asarray(K_w_segments_set, dtype=int)\n",
    "    if J_x_segments_set.size != K_w_segments_set.size:\n",
    "        raise ValueError(\"J_x_segments_set and K_w_segments_set must have the same length.\")\n",
    "\n",
    "    # Build all ordered pairs (J1, J2) with J2 > J1\n",
    "    J_pairs = []\n",
    "    for a in J_x_segments_set:\n",
    "        for b in J_x_segments_set:\n",
    "            if b > a:\n",
    "                J_pairs.append((int(a), int(b)))\n",
    "    J_pairs = np.array(J_pairs, dtype=int)\n",
    "    num_pairs = J_pairs.shape[0]\n",
    "\n",
    "    # Map each J in X-set to its corresponding K in W-set by index matching on value\n",
    "    # (assumes both sets are aligned by value; duplicates would be ambiguous)\n",
    "    J_to_K = {int(J_x_segments_set[i]): int(K_w_segments_set[i]) for i in range(J_x_segments_set.size)}\n",
    "    K_pairs = np.array([(J_to_K[a], J_to_K[b]) for (a, b) in J_pairs], dtype=int)\n",
    "\n",
    "    # Storage\n",
    "    Z_sup = np.empty(num_pairs)\n",
    "    Z_sup_boot = np.full((boot_num, num_pairs), np.nan)\n",
    "\n",
    "    it_pairs = trange(num_pairs, disable=not progress, desc=\"complexity determination\")\n",
    "\n",
    "    for ii in it_pairs:\n",
    "        J1_seg, J2_seg = int(J_pairs[ii, 0]), int(J_pairs[ii, 1])\n",
    "        K1_seg, K2_seg = int(K_pairs[ii, 0]), int(K_pairs[ii, 1])\n",
    "\n",
    "        if K_w_degree < J_x_degree:\n",
    "            raise ValueError(\"K_w_degree must be >= J_x_degree\")\n",
    "\n",
    "        # --- W bases (J1/J2 paths)\n",
    "        if K_w_degree == 0:\n",
    "            B_w_J1 = B_w_J2 = np.ones((W.shape[0], 1))\n",
    "        else:\n",
    "            K_mat_W1 = np.column_stack([np.repeat(K_w_degree, W.shape[1]), np.repeat(K1_seg, W.shape[1])])\n",
    "            K_mat_W2 = np.column_stack([np.repeat(K_w_degree, W.shape[1]), np.repeat(K2_seg, W.shape[1])])\n",
    "            B_w_J1 = prodspline(x=W, K=K_mat_W1, knots=knots, basis=basis, x_min=W_min, x_max=W_max)\n",
    "            B_w_J2 = prodspline(x=W, K=K_mat_W2, knots=knots, basis=basis, x_min=W_min, x_max=W_max)\n",
    "            if basis != \"tensor\":\n",
    "                B_w_J1 = np.column_stack([np.ones((W.shape[0], 1)), B_w_J1])\n",
    "                B_w_J2 = np.column_stack([np.ones((W.shape[0], 1)), B_w_J2])\n",
    "\n",
    "        # --- X bases (J1/J2 paths) at sample + grid eval\n",
    "        if J_x_degree == 0:\n",
    "            Psi_x_J1 = Psi_x_J2 = np.ones((X.shape[0], 1))\n",
    "            Psi_x_J1_eval = Psi_x_J2_eval = np.ones((X_grid.shape[0], 1))\n",
    "        else:\n",
    "            K_mat_X1 = np.column_stack([np.repeat(J_x_degree, X.shape[1]), np.repeat(J1_seg, X.shape[1])])\n",
    "            K_mat_X2 = np.column_stack([np.repeat(J_x_degree, X.shape[1]), np.repeat(J2_seg, X.shape[1])])\n",
    "            Psi_x_J1 = prodspline(x=X, K=K_mat_X1, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "            Psi_x_J2 = prodspline(x=X, K=K_mat_X2, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "            Psi_x_J1_eval = prodspline(x=X, xeval=X_grid, K=K_mat_X1, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "            Psi_x_J2_eval = prodspline(x=X, xeval=X_grid, K=K_mat_X2, knots=knots, basis=basis, x_min=X_min, x_max=X_max)\n",
    "            if basis != \"tensor\":\n",
    "                Psi_x_J1 = np.column_stack([np.ones((X.shape[0], 1)), Psi_x_J1])\n",
    "                Psi_x_J2 = np.column_stack([np.ones((X.shape[0], 1)), Psi_x_J2])\n",
    "                Psi_x_J1_eval = np.column_stack([np.ones((X_grid.shape[0], 1)), Psi_x_J1_eval])\n",
    "                Psi_x_J2_eval = np.column_stack([np.ones((X_grid.shape[0], 1)), Psi_x_J2_eval])\n",
    "\n",
    "        # --- 2SLS algebra per J ---\n",
    "        BtB_inv_1 = _ginv(B_w_J1.T @ B_w_J1)\n",
    "        P_B1 = B_w_J1 @ BtB_inv_1 @ B_w_J1.T\n",
    "        PsiTP1 = Psi_x_J1.T @ P_B1\n",
    "        tmp1 = _ginv(PsiTP1 @ Psi_x_J1) @ PsiTP1\n",
    "        beta1 = tmp1 @ Y\n",
    "        U1 = Y - (Psi_x_J1 @ beta1)\n",
    "        hhat_J1 = Psi_x_J1_eval @ beta1\n",
    "\n",
    "        BtB_inv_2 = _ginv(B_w_J2.T @ B_w_J2)\n",
    "        P_B2 = B_w_J2 @ BtB_inv_2 @ B_w_J2.T\n",
    "        PsiTP2 = Psi_x_J2.T @ P_B2\n",
    "        tmp2 = _ginv(PsiTP2 @ Psi_x_J2) @ PsiTP2\n",
    "        beta2 = tmp2 @ Y\n",
    "        U2 = Y - (Psi_x_J2 @ beta2)\n",
    "        hhat_J2 = Psi_x_J2_eval @ beta2\n",
    "\n",
    "        # --- Asymptotic variances ---\n",
    "        Wgt1 = (tmp1.T * U1.ravel()[:, None])     # n x J1\n",
    "        D1 = Wgt1.T @ Wgt1                         # J1 x J1\n",
    "        asy_var_J1 = np.sum((Psi_x_J1_eval @ D1) * Psi_x_J1_eval, axis=1)  # (neval,)\n",
    "\n",
    "        Wgt2 = (tmp2.T * U2.ravel()[:, None])     # n x J2\n",
    "        D2 = Wgt2.T @ Wgt2                         # J2 x J2\n",
    "        asy_var_J2 = np.sum((Psi_x_J2_eval @ D2) * Psi_x_J2_eval, axis=1)\n",
    "\n",
    "        # --- Covariance term ---\n",
    "        # A1: J1 x n, B2: n x J2\n",
    "        A1 = (tmp1.T * U1.ravel()[:, None]).T      # J1 x n\n",
    "        B2 = (tmp2.T * U2.ravel()[:, None])        # n x J2\n",
    "        M12 = A1 @ B2                               # J1 x J2\n",
    "        S = (Psi_x_J1_eval @ M12)                   # neval x J2\n",
    "        asy_cov_J1J2 = np.sum(S * Psi_x_J2_eval, axis=1)  # (neval,)\n",
    "\n",
    "        # denom (vector) and sup t-stat\n",
    "        asy_se = np.sqrt(np.maximum(0.0, asy_var_J1 + asy_var_J2 - 2.0 * asy_cov_J1J2))\n",
    "        Z_sup[ii] = float(np.max(np.abs((hhat_J1.ravel() - hhat_J2.ravel()) / _nzd(asy_se))))\n",
    "\n",
    "        # --- Bootstrap sup t (same draws across columns not needed here) ---\n",
    "        it = trange(boot_num, disable=not progress, desc=f\"boot sup t ({ii+1}/{num_pairs})\")\n",
    "        for b in it:\n",
    "            z = np.random.standard_normal(n)\n",
    "            num = (Psi_x_J1_eval @ (tmp1 @ (U1.ravel() * z).reshape(-1, 1))) \\\n",
    "                - (Psi_x_J2_eval @ (tmp2 @ (U2.ravel() * z).reshape(-1, 1)))\n",
    "            Z_sup_boot[b, ii] = np.max(np.abs(num.ravel() / _nzd(asy_se)))\n",
    "\n",
    "    # Max over pairs for each bootstrap draw -> critical value\n",
    "    Z_boot = np.nanmax(Z_sup_boot, axis=1)\n",
    "    # R: quantile(..., type=5). We use NumPy's default linear interpolation.\n",
    "    theta_star = float(np.quantile(Z_boot, 1 - alpha, method=\"linear\"))\n",
    "\n",
    "    # ----- Lepski choice -----\n",
    "    num_J = J_x_segments_set.size\n",
    "    test_mat = np.full((num_J, num_J), np.nan)\n",
    "    # Fill only upper triangle (j2 > j1) by mapping pairs back to indices\n",
    "    J_index = {int(v): i for i, v in enumerate(J_x_segments_set)}\n",
    "    for ii, (j1, j2) in enumerate(J_pairs):\n",
    "        r = J_index[int(j1)]\n",
    "        c = J_index[int(j2)]\n",
    "        test_mat[r, c] = 1.0 if Z_sup[ii] <= 1.1 * theta_star else 0.0\n",
    "\n",
    "    # Per-row product over strictly larger columns; use NA rules like R:\n",
    "    test_vec = np.full(num_J, np.nan)\n",
    "    for i in range(num_J - 1):\n",
    "        row = test_mat[i, i + 1 :]\n",
    "        if row.size == 0:\n",
    "            test_vec[i] = np.nan\n",
    "        else:\n",
    "            valid = row[~np.isnan(row)]\n",
    "            if valid.size == 0:\n",
    "                test_vec[i] = np.nan\n",
    "            else:\n",
    "                test_vec[i] = 1.0 if np.all(valid == 1.0) else 0.0\n",
    "\n",
    "    # Choose segment J.seg\n",
    "    if np.all(np.isnan(test_vec) | (test_vec == 0.0)):\n",
    "        J_seg = int(np.max(J_x_segments_set))\n",
    "    elif np.any(test_vec == 1.0):\n",
    "        # smallest J for which row passes\n",
    "        first_ok = int(np.where(test_vec == 1.0)[0][0])\n",
    "        J_seg = int(J_x_segments_set[first_ok])\n",
    "    else:\n",
    "        J_seg = int(np.min(J_x_segments_set))\n",
    "\n",
    "    # Dimension for chosen J\n",
    "    J_hat = int(dimbs(basis=basis,\n",
    "                      degree=[J_x_degree] * p,\n",
    "                      segments=[J_seg] * p))\n",
    "\n",
    "    # Truncated (second-largest) segments and its \"dimension\" proxy\n",
    "    if num_J >= 2:\n",
    "        J_seg_n = int(np.sort(J_x_segments_set)[-2])\n",
    "    else:\n",
    "        J_seg_n = int(J_seg)\n",
    "    J_hat_n = int((J_seg_n + J_x_degree) ** p)\n",
    "\n",
    "    # Final J.x.seg, J.tilde, K.w.seg (paired by matching J)\n",
    "    J_x_seg = int(min(J_seg, J_seg_n))\n",
    "    J_tilde = int(min(J_hat, J_hat_n))\n",
    "    K_w_seg = int(J_to_K[J_x_seg])\n",
    "\n",
    "    return dict(\n",
    "        J.tilde=J_tilde,\n",
    "        J.hat=J_hat,\n",
    "        J.hat.n=J_hat_n,\n",
    "        J.x.seg=J_x_seg,\n",
    "        K.w.seg=K_w_seg,\n",
    "        theta.star=theta_star,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86016ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Iterable, Dict, Any, Callable\n",
    "\n",
    "\n",
    "# --------- small helpers (R analogues) ---------\n",
    "\n",
    "def _match_arg(val: str, choices, name: str) -> str:\n",
    "    choices = list(choices)\n",
    "    if val not in choices:\n",
    "        raise ValueError(f\"{name} must be one of {choices}, got {val!r}.\")\n",
    "    return val\n",
    "\n",
    "def _ginv(A: np.ndarray) -> np.ndarray:\n",
    "    return np.linalg.pinv(A, rcond=1e-12)\n",
    "\n",
    "def _is_fullrank(M: np.ndarray) -> bool:\n",
    "    if M.ndim == 1:\n",
    "        M = M[:, None]\n",
    "    return np.linalg.matrix_rank(M) == M.shape[1]\n",
    "\n",
    "def _as_colvec(y: Iterable[float]) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    return y.reshape(-1, 1)\n",
    "\n",
    "def _nzd(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    # Non-zero denominator safeguard (R's NZD)\n",
    "    return np.where(np.abs(x) < eps, eps, x)\n",
    "\n",
    "def _identical(A: np.ndarray, B: np.ndarray) -> bool:\n",
    "    return np.array_equal(np.asarray(A), np.asarray(B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, Callable\n",
    "\n",
    "\n",
    "def npiv_choose_J(\n",
    "    Y,\n",
    "    X: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    X_grid: Optional[np.ndarray] = None,\n",
    "    J_x_degree: int = 3,\n",
    "    K_w_degree: int = 4,\n",
    "    K_w_smooth: int = 2,\n",
    "    knots: str = \"uniform\",                 # {\"uniform\",\"quantiles\"}\n",
    "    basis: str = \"tensor\",                  # {\"tensor\",\"additive\",\"glp\"}\n",
    "    X_min: Optional[float] = None,\n",
    "    X_max: Optional[float] = None,\n",
    "    W_min: Optional[float] = None,\n",
    "    W_max: Optional[float] = None,\n",
    "    grid_num: int = 50,\n",
    "    boot_num: int = 99,\n",
    "    check_is_fullrank: bool = False,\n",
    "    progress: bool = True,\n",
    "    *,\n",
    "    prodspline: Callable[..., np.ndarray],\n",
    "    dimbs: Callable[..., int],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Python port of R's `npiv_choose_J`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    (Same meaning as in R.)\n",
    "    You must inject:\n",
    "      - prodspline(x, K, knots, basis, x_min, x_max, xeval=None, deriv_index=None, deriv=None) -> ndarray\n",
    "      - dimbs(basis, degree, segments) -> int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "      J.hat.max, J.hat.n, J.hat, J.tilde, J.x.seg, K.w.seg,\n",
    "      J.x.segments.set, K.w.segments.set, theta.star\n",
    "    \"\"\"\n",
    "    # Step 1: compute \\hat{J}_max and the data-driven grids of segments for X and W\n",
    "    tmp1 = npiv_Jhat_max(\n",
    "        X=X,\n",
    "        W=W,\n",
    "        J_x_degree=J_x_degree,\n",
    "        K_w_degree=K_w_degree,\n",
    "        K_w_smooth=K_w_smooth,\n",
    "        knots=knots,\n",
    "        basis=basis,\n",
    "        X_min=X_min,\n",
    "        X_max=X_max,\n",
    "        W_min=W_min,\n",
    "        W_max=W_max,\n",
    "        check_is_fullrank=check_is_fullrank,\n",
    "        progress=progress,\n",
    "        prodspline=prodspline,\n",
    "        dimbs=dimbs,\n",
    "    )\n",
    "\n",
    "    # Step 2: run Lepski/bootstrapped choice over that grid\n",
    "    tmp2 = npiv_J(\n",
    "        Y=Y,\n",
    "        X=X,\n",
    "        W=W,\n",
    "        X_grid=X_grid,\n",
    "        J_x_degree=J_x_degree,\n",
    "        K_w_degree=K_w_degree,\n",
    "        J_x_segments_set=tmp1[\"J.x.segments.set\"],\n",
    "        K_w_segments_set=tmp1[\"K.w.segments.set\"],\n",
    "        knots=knots,\n",
    "        basis=basis,\n",
    "        X_min=X_min,\n",
    "        X_max=X_max,\n",
    "        W_min=W_min,\n",
    "        W_max=W_max,\n",
    "        grid_num=grid_num,\n",
    "        boot_num=boot_num,\n",
    "        alpha=tmp1[\"alpha.hat\"],\n",
    "        check_is_fullrank=check_is_fullrank,\n",
    "        progress=progress,\n",
    "        prodspline=prodspline,\n",
    "        dimbs=dimbs,\n",
    "    )\n",
    "\n",
    "    # Step 3: return combined results\n",
    "    return {\n",
    "        \"J.hat.max\": tmp1[\"J.hat.max\"],\n",
    "        \"J.hat.n\": tmp2[\"J.hat.n\"],\n",
    "        \"J.hat\": tmp2[\"J.hat\"],\n",
    "        \"J.tilde\": tmp2[\"J.tilde\"],\n",
    "        \"J.x.seg\": tmp2[\"J.x.seg\"],\n",
    "        \"K.w.seg\": tmp2[\"K.w.seg\"],\n",
    "        \"J.x.segments.set\": tmp1[\"J.x.segments.set\"],\n",
    "        \"K.w.segments.set\": tmp1[\"K.w.segments.set\"],\n",
    "        \"theta.star\": tmp2[\"theta.star\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66bd222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n",
      "1.3399985618889332e-05 npiv [1.]\n"
     ]
    }
   ],
   "source": [
    "# Example for tensor product:\n",
    "n = 4\n",
    "X1 = np.column_stack([np.ones(n), np.linspace(0,1,n)])\n",
    "X2 = np.column_stack([np.ones(n), np.linspace(1,2,n), np.linspace(1,2,n)**2])\n",
    "T = tensor_prod_model_matrix([X1, X2])\n",
    "print(T.shape)  # (n, 2*3) = (4, 6)\n",
    "\n",
    "# Example wrapper call:\n",
    "def dummy_npiv_est(**kw):\n",
    "    # pretend we fit a model; return something minimal\n",
    "    return {\"coef\": np.array([1.0]), \"se\": np.array([0.1])}\n",
    "\n",
    "res = npiv_default(\n",
    "    Y=np.random.randn(n),\n",
    "    X=X1,\n",
    "    W=X2,\n",
    "    npiv_est=dummy_npiv_est,\n",
    ")\n",
    "print(res[\"ptm\"], res[\"_class\"], res[\"coef\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
